\documentclass{article}


\include{stddefs}
\include{imodefs}

\newcommand{\vect}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\newcommand{\vects}[3]{\begin{pmatrix} #1 \\ #2 \\ #3\end{pmatrix}}
\newcommand{\hphm}{\hphantom{-}}


\chapterno{5}

\begin{document}

\chapter{Euclidean vector spaces}

Big data are made up of many numbers in data sets. Such data sets can
be represented as vectors in a high dimensional euclidean vector
space. A vector is nothing but a list of numbers, but we need to talk
mathematically about the size of a vector and perform operations on
vectors. The purpose of this chapter is to set the stage for this,
especially by introducing the dot product (or inner product) for
vectors. Having a dot product is immensely useful and we give several
applications like linear regression and the perceptron learning algorithm

In the last part of the chapter rudimentary basics of analysis
are introduced like sequences, continuous functions, open, closed
and compact subsets. Some results will in this context only
be quoted and not proved.


\section{Vectors in the plane}

The dot product (or inner product) between two vectors $u, v\in \RR^2$ is
given by
\begin{equation}\label{smhack}
u\cdot v = x_1 x_2 + y_1 y_2,
\end{equation}
where
\begin{equation}
u = \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}\qquad\text{and}\qquad
v = \begin{pmatrix} x_2 \\ y_2 \end{pmatrix}.
\end{equation}
We may also interpret
$u$ and $v$  as $2\times 1$ matrices (or column vectors).
Then the dot product in
\eqref{smhack} may be realized as the matrix product:
$$
u\cdot v = u^T v.
$$
The length or \emph{norm} of the vector $u\in \RR^2$ is given by
$$
\abs{u} = \sqrt{u\cdot u} = \sqrt{u^T u} = \sqrt{x_1^2 + y_1^2}.
$$
This follows from the \url{Pythagorean theorem}{https://en.wikipedia.org/wiki/Pythagorean_theorem}:

\includegraphics{Pythagoras.svg}


Also, the cosine of the angle $\theta$ between $u$ and $v$ is given by
$$
\cos(\theta) = \frac{u\cdot v}{\abs{u}\abs{v}}\qquad\text{or}\qquad u \cdot v = \abs{u} \abs{v} \cos(\theta).
$$
We will not go into this formula. It is a byproduct of considering the projection a vector on
another vector (see Exercise \ref{dim2vec}).

\section{Higher dimensions}

The notions of dot product, norm and the formula for cosine of the
angle generalize immediately to vectors in dimensions
higher than two.

We denote the set of column vectors with $d$ rows
by $\RR^d$ and call it the euclidean vector space of
dimension $d$. An element $v\in \RR^d$ is called a vector and it has
the form (column vector with $d$ entries)
$$
v =
\begin{pmatrix}
  x_1 \\
  x_2 \\
  \vdots
  \\
  x_d
\end{pmatrix}.
$$

A vector in $\RR^d$ is a model for a data set in real life. A collection
of $d$ numbers, which could signify $d$ measurements. You will see
an example of this below, where a vector represents a data set
counting words in a string.

Being column vectors, vectors in $\RR^d$ can be added and multiplied by
numbers:
$$
\begin{pmatrix}
  x_1 \\
  x_2 \\
  \vdots
  \\
  x_d
\end{pmatrix} + 
\begin{pmatrix}
  y_1 \\
  y_2 \\
  \vdots
  \\
  y_d
\end{pmatrix}
=
\begin{pmatrix}
  x_1 + y_1 \\
  x_2 + y_2\\
  \vdots
  \\
  x_d + y_d
\end{pmatrix}\qquad\qquad
\lambda
\begin{pmatrix}
  x_1 \\
  x_2 \\
  \vdots
  \\
  x_d
\end{pmatrix} =
\begin{pmatrix}
  \lambda x_1 \\
  \lambda x_2 \\
  \vdots
  \\
  \lambda x_d
\end{pmatrix}.
$$

The dot product generalizes as follows to higher dimensions.

\subsection{Dot product and norm}

\begin{definition}
The dot product of
$$
u =
\begin{pmatrix}
  x_1 \\
  x_2 \\
  \vdots
  \\
  x_d
\end{pmatrix}\qquad
\text{and}
\qquad
u =
\begin{pmatrix}
  y_1 \\
  y_2 \\
  \vdots
  \\
  y_d
\end{pmatrix}
$$
is defined by
\begin{equation}\label{dotpr}
u\cdot v = u^T v = x_1 y_1 + x_2 y_2 + \cdots + x_d y_d.
\end{equation}


The norm of $u\in \RR^d$ is defined by
\begin{equation}\label{vecnorm}
\abs{u} = \sqrt{u\cdot u} = \sqrt{x_1^2 + x_2^2 + \cdots + x_d^2}.
\end{equation}


A vector $e\in \RR^d$ with $\abs{e}=1$ is called a \emph{unit vector}.

Two vectors $u, v\in \RR^d$ are called \emph{orthogonal} if $u\cdot v = 0$. We write
this as $u \perp v$.

\end{definition}


\beginshex\label{normrules}
Use the definition in \eqref{dotpr} to show that
\begin{align*}
  u\cdot (v + w) &= u\cdot v + u\cdot w\\
  (\lambda u)\cdot v &= u\cdot (\lambda v) = \lambda (u\cdot v)
\end{align*}
for $u, v, w\in \RR^d$ and $\lambda\in\RR$.
\endshex


\beginshex
Let $u\in \RR^d$ be a nonzero vector and $\lambda\in \RR$. Use the definition
in \eqref{vecnorm} to show that
$|\lambda u| = |\lambda| \, |u|$ and that
$$
\frac{1}{|u|} u
$$
is a unit vector.

\begin{hideinbutton}{Hint}
You could perhaps use Exercise \ref{normrules} to do this.   
\end{hideinbutton}
\endshex



\beginshex\label{dim2vec}
Given two vectors $u, v\in \RR^d$ with $v\neq 0$, find $\lambda\in \RR$, such
that $u - \lambda v$ and $v$ are orthogonal, i.e.
$$
(u - \lambda v) \cdot v = 0.
$$

For $d=2$,
sketch that if $u - \lambda v$ and $v$ are orthogonal, then
$u, \lambda v$ and $u-\lambda v$ are the sides in a right triangle.
In this case, if $\theta$ is the angle between $u$ and $v$, show that
$$
\abs{u} \cos(\theta) = \abs{v} \lambda.
$$
Use this to show that
$$
u\cdot v = \abs{u} \abs{v} \cos(\theta).
$$
Finally show that
$$
\cos(A - B) = \cos(A) \cos(B) + \sin(A) \sin(B),
$$
where $A$ and $B$ are two angles.

\begin{hint}[showhide]
In the last question, you could use that the vectors
$$
\begin{pmatrix}
  \cos(A)\\ \sin(A)
\end{pmatrix}\qquad\text{and}\qquad
\begin{pmatrix}
  \cos(B)\\ \sin(B)
\end{pmatrix}
$$
are unit vectors.
\end{hint}
\endshex


\beginshex
Given two vectors $u, v\in \RR^d$, solve the minimization problem
\begin{align*}
  &\text{Minimize} &\abs{u - \lambda v}&\\
  &\text{with constraint}\\
  &&\lambda\in\RR.
\end{align*}

\begin{hint}[showhide]
  First convince yourself that $\lambda$ minimizes $\abs{u - \lambda v}$
  if and only if it minimizes
  $$
  (u - \lambda v)\cdot (u - \lambda v) = \abs{v}^2 \lambda^2 - 2 (u\cdot v)\lambda + \abs{u}^2,
  $$
  which happens to be a quadratic polynomial in $\lambda$.
\end{hint}
\endshex

\begin{quizexercise}[showhide]
\begin{quiz}
\question
Let $d$ denote the distance from $(1, 1)$ to the line through $(0, 0)$ and $(2, 1)$. What is true about $d$?
\answer{F}
$$
d = \frac{1}{2}.
$$
\answer{F}
$$
d = 0.447214.
$$
\answer{T}
$$
d = \frac{\sqrt{5}}{5}.
$$
\answer{F}
$$
d = \frac{2}{\sqrt{5}}.
$$
\end{quiz}
\end{quizexercise}


\begin{example}\label{exampleperceptronsimple}

Already at this point we have the necessary definitions for sneak previewing the
perceptron algorithm outlined more generally at the end of the chapter.
In terms of the dot product, the central problem turns into the following:

%\begin{quote}
  Given finitely many vectors $v_1, \dots, v_n\in \RR^d$, can we find
  $\alpha\in \RR^d$, such that
  $$
  \alpha\cdot v_i > 0
  $$
  for every $i = 1, \dots, n$?
%\end{quote}

  
\beginshex
Come up with a simple example, where this problem is unsolvable.
\endshex

\beginshex
Suppose that there exists $\alpha\in \RR^d$, such that
$\alpha\cdot v_i > 0$ for every $i = 1, \dots, n$. Show then that
there exists $\alpha^*\in \RR^d$, such that
$$
\alpha^* \cdot v_i \geq 1
$$
for every $i = 1, \dots, n$.

\begin{hideinbutton}{Hint}
  Let $\mu = \min(\alpha\cdot v_1, \dots, \alpha\cdot v_n)$. Show that
  $\alpha^* = \frac{1}{\mu} \alpha$ works.
\end{hideinbutton}
\endshex

In case the problem is solvable, the following
ridiculously simple algorithm works for finding $\alpha$:

\begin{frameit}
  \begin{enumerate}[(i)]
  \item Begin by putting $\alpha = 0$.
  \item If there exists $v_i\in \{v_1, \dots, v_n\}$ with $\alpha\cdot v_i \leq 0$, then replace
    $\alpha$ by $\alpha + v_i$ and repeat this step. Otherwise $\alpha$ is the desired output vector.
  \end{enumerate}
    
\end{frameit}


% \begin{enumerate}
% \item Set $\alpha := 0$.
% \item If $\alpha \cdot v_i \leq 0$ for some $v_i$, then set $\alpha := \alpha + v_i$ and repeat this step. Otherwise stop.
% \end{enumerate}

Let us try out the algorithm on the simple example of just
two points in $\RR^2$ given by
$$
v_1 = \vect{-1}{\hphm 1}\qquad\text{and}\qquad v_2 =
\vect{1}{0}.
$$

In this case the algorithm proceeds as pictured below.



$$
\alpha = \vect{0}{0}\xrightarrow{+v_1} \vect{-1}{\hphm 1} \xrightarrow{+v_2} \vect{0}{1} \xrightarrow{+v_2}\vect{1}{1} \xrightarrow{+v_1}\vect{0}{2} \xrightarrow{+v_2} \vect{1}{2}.
$$

It patiently crawls its way ending with the vector $\alpha = \vect{1}{2}$, which
satisfies $\alpha\cdot v_1 > 0$ and $\alpha\cdot v_2 > 0$.

\includegraphics{perceptronsimple.png}

\end{example}

\subsection{Pythagoras and the least squares method}

The result below is a generalization of the theorem of Pythagoras about right
triangles to higher dimensions.

\begin{proposition}\label{proppythagoras}
  If $u, v\in \RR^d$ and $u\perp v$, then
  $$
  \abs{u + v}^2 = \abs{u}^2 + \abs{v}^2.
  $$
\end{proposition}

  \begin{proof}[showhide]
    This follows from
    $$
    (u + v)\cdot(u+v) = u\cdot u + u\cdot v + v\cdot u + v\cdot v = u\cdot u + v\cdot v = \abs{u}^2 + \abs{v}^2,
    $$
    since $u \cdot v = v\cdot u = 0$.
  \end{proof}




The dot product and the norm have a vast number of applications. One of them is the
method of least squares: suppose that you are presented with a system
\begin{equation}\label{lineqslsq}
A x = b
\end{equation}
of linear equations, where $A$ is an $m\times n$ matrix.

You may not be able to solve \eqref{lineqslsq}. There could be for example
$17$ equations and only $2$ unknowns making it impoosible for all the equations to hold.
As an example, the system

\newcommand{\mph}{\phantom{-}}

\begin{equation}\label{nosols}
\begin{pmatrix}
\mph 1 & \mph 1\\
\mph 1 & -1\\
-1 & \mph 1
\end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} = \begin{pmatrix} 3 \\ 1 \\ 1 \end{pmatrix}
\end{equation}
of three linear equations and two unknowns does not have any solutions.


The method of (linear) least squares seeks the best approximate solution $x_0$ to \eqref{lineqslsq} as a
solution to the minimization problem



\begin{align}\label{optlsq}
  &\text{Minimize} &\abs{b - A x}^2&\\
  &\text{with constraint}\\
  &&x\in\RR^n.
\end{align}

There is a surprising way of finding optimal solutions to \eqref{optlsq}:



\begin{theorem}\label{lsqthm}
  If $x_0\in\RR^n$ is a solution to the system
  $$
  (A^T A) x = A^T b
  $$
  of $n$ linear equations with $n$ unknowns,
  then $x_0$ is an optimal solution to \eqref{optlsq}.
\end{theorem}

  \begin{proof}[showhide]
    Suppose we know that $b - A x_0$ is orthogonal to $A v$ for every $v\in \RR^n$. Then
    $$
    \abs{b - A x}^2 = \abs{b - A x_0 + A(x_0 - x)}^2 = \abs{b - A x_0}^2 + \abs{A(x-x_0)}^2
    $$
    for every $x\in \RR^n$
    by Proposition \ref{proppythagoras}. So, in the case that
    $b - A x_0 \perp A v$ for every $v\in \RR^n$ we have
    $$
    \abs{b - A x}^2 \geq \abs{b - A x_0}^2
    $$
    for every $x\in \RR^n$ proving that $x_0$ is an optimal solution to
    \eqref{optlsq}.

    Now $b - A x_0$ is orthogonal to $A v$ for every $v\in \RR^n$ if and
    only if $b - A x_0$ is orthogonal to all of the column vectors in $A$. This is satisfied if and only if the dot products of $b - A x_0$ with all
    the column vectors of $A$ are zero, which translates into
    $$
    A^T(b - A x_0) = 0
    $$
    or $(A^T A) x_0 = A^T b$.
  \end{proof}

In a future course on linear algebra you will see that the system of linear equations in
Theorem \ref{lsqthm} is always solvable i.e., an optimal solution to \eqref{optlsq} can
always be found in this way.


\beginshex
Show that \eqref{nosols} has no solutions. Compute the best approximate solution to \eqref{nosols} 
using Theorem \ref{lsqthm}.
\endshex

\begin{example}
  The classical application of the least squares method is to find
the best line $y = \alpha x + \beta$ through a given set of points
$$
(x_1, y_1), \quad (x_1, y_1), \quad \dots \quad, (x_n, y_n)
$$
in the plane $\RR^2$.

Usually we cannot find a line matching the points precisely. This corresponds to the fact that
the system of equations
$$
\begin{pmatrix}
x_1 & 1\\
x_2 & 1\\
\vdots & \vdots\\
x_n & 1
\end{pmatrix}
\begin{pmatrix}
\alpha \\ \beta
\end{pmatrix}
= 
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
$$
has no solutions.

Working with the least squares solution, we try to compute the best
line $y = \alpha x + \beta$ in the sense that
$$
(y_1 -\alpha x_1 -\beta)^2 + (y_2 -\alpha x_2 -\beta)^2 + \cdots +
(y_n -\alpha x_n -\beta)^2 
$$
is minimized.

\begin{figure}
\includegraphics{linreg.png}
\caption{Best fit of line to random points from \url{Wikipedia}{https://en.wikipedia.org/wiki/Least_squares}.}
\end{figure}

We might as well have asked for the best quadratic polynomial 
$$
y = \alpha x^2 + \beta x + \gamma 
$$
passing through the points
$$
(x_1, y_1), \quad (x_1, y_1), \quad \dots \quad, (x_n, y_n)
$$
in $\RR^2$.

The same method gives us the system
$$
\begin{pmatrix}
x_1^2 & x_1 & 1\\
x_2 ^2 & x_2 & 1\\
\vdots & \vdots\\
x_n^2 & x_n & 1
\end{pmatrix}
\begin{pmatrix}
\alpha \\ \beta \\ \gamma
\end{pmatrix}
= 
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
$$
of linear equations.


\begin{figure}
\includegraphics{parabelfit.png}
\caption{Best fit of quadratic polynomial to random points from  \url{Wikipedia}{https://en.wikipedia.org/wiki/Least_squares}.}
\end{figure}



The method generalizes naturally to finding the best polynomial of degree $m$
$$
y = a_m x^m + a_{m-1} x^{m-1} + \cdots + a_1 x + a_0 
$$
through a given set of points.
\end{example}

\beginshex
Find the best line $y = \alpha x + \beta$ through the points
$(1, 2), (2,1)$ og $(4,3)$ and the best quadratic polynomial
$ y = a x^2 + b x + c$ through the points
$(-2, 2), (-1, 1), (0,0), (1,1)$ and $(2,2)$.

It is important here, that you write down the relevant system
of linear equations according to Theorem \ref{lsqthm}.
It is however ok to solve the equations
on a computer (or check your best fit on \url{WolframAlpha}{https://www.wolframalpha.com/}).

Also, you can get a graphical illustration of your result in the sage window below.

\begin{sage}
pts = [(-2, 2), (-1, 1), (0, 0), (1, 1), (2, 2)]
a = 1
b = 1
c = 1
show(points(pts, pointsize=30) + plot(a*x^2 + b*x + c, (x, -2.5, 2.5)))
\end{sage}
\endshex


\beginshex
A circle with center $(a, b)$ and radius $r$ is given by the equation

\begin{equation}\label{eq:circle}
(x - a)^2 + (y - b)^2 = r^2.
\end{equation}

\begin{enumerate}
\item
  Explain how \eqref{eq:circle} can be rewritten to the equation
\begin{equation}\label{eq:circfit}
2 a x + 2 b y + c = x^2 + y^2,
\end{equation}
where $c = r^2 - a^2 - b^2$.

\item
  Explain how fitting a circle to the points $(x_1, y_1), \dots, (x_n, y_n)$
  in the least squares context using \eqref{eq:circfit} leads to the system
$$
\begin{pmatrix}
2 x_1 & 2 y_1 & 1\\
2 x_2 & 2 y_2 & 1\\
\vdots &\vdots &\vdots\\
2 x_n & 2 y_n & 1
\end{pmatrix}
\begin{pmatrix}
a \\
b \\
c
\end{pmatrix}
=
\begin{pmatrix}
x_1^2 + y_1^2\\
x_2^2 + y_2^2\\
\vdots \\
x_n^2 + y_n^2
\end{pmatrix},
$$
of linear equations.

\item

  Compute the best circle through the points
$$
(0, 2),\quad (0, 3),\quad  (2,0)\quad\text{og}\quad (3, 1)
$$ 
by giving the center coordinates and radius with two decimals.

\includegraphics{circlefit.png}

\end{enumerate}

\begin{sage}
pts = [(0, 2), (0, 3), (2, 0), (3, 1)]

x = 2
y = 2
r = 2

show(points(pts, pointsize=30) + circle((x, y), r))
\end{sage}

\endshex

\subsection{The Cauchy-Schwarz inequality}

Even though the generalizations of the dot product and norm to higher
dimensions amount to just adding some coordinates, they entail
a rather stunning result called the \url{Cauchy-Schwarz inequality}{https://en.wikipedia.org/wiki/Cauchy\%E2\%80\%93Schwarz_inequality}. The
proof is not long, but revolves around a rather beautiful trick.


\begin{theorem}\label{cs}
  For two vectors $u, v\in \RR^d$,
  $$
  \abs{u\cdot v} \leq \abs{u} \abs{v}.
  $$
\end{theorem}
  \begin{proof}[showhide]
    We consider the function $q:\RR\rightarrow \RR$ given by
    $$
    q(x) = (x u + v)\cdot (x u + v) = \abs{u}^2 x^2 + 2 (u\cdot v) x + \abs{v}^2
    $$
    Then $q(x)$ is a quadratic polynomial with $q(x)\geq 0$. Therefore
    its discriminant must be $\leq 0$ i.e.,
    $$
    4 (u\cdot v)^2 - 4 \abs{u}^2 \abs{v}^2 \leq 0,
    $$  
    which gives the result.
  \end{proof}


The Cauchy-Schwarz inequality
implies that 
$$
-1 \leq \frac{u\cdot v}{|u|\, |v|} \leq 1
$$
for two vectors $u, v\in \RR^d$ and it makes sense to define the angle
$\theta$ between these vectors\index{angle between vectors} by
\begin{equation}\label{cossim}
\cos(\theta) = 
\frac{u \cdot v}{|u|\, |v|}.
\end{equation}

\beginshex
It seems that
$$
2 (x^2 + y^2) \geq (x + y)^2
$$
for arbitrary two numbers $x, y\in \RR$, since
$$
2(x^2 + y^2) - (x + y)^2 = x^2 + y^2 - 2 x y = (x - y)^2.
$$

Why is
$$
n (x_1^2 + \cdots + x_n^2) \geq (x_1 + \cdots + x_n)^2
$$
for arbitary $n$ numbers $x_1, \dots, x_n\in \RR$?
\endshex

\begin{example}

  When vectors are interpreted as data sets,
  the number in \eqref{cossim} is known as the \emph{cosine similarity}
and measures the correlation between the two data sets
$u$ and $v$.

An application could be the similarity between two strings.
Consider the two strings 
"\emph{Mathematics is fun and matrices are useful}"
and
"\emph{Mathematics is fun and matrices are applicable}".

%
%\begin{html}
% <ul>
% <li>Mathematics is fun and matrices are useful</li>
% <li>Mathematics is fun and matrices are applicable</li>
% </ul>
% \end{html}
From the words in the two strings we form the following vectors in $\RR^8$.

$$
\begin{array}{lll}
  \text{Mathematics} & 1 & 1\\[0.5em]
  \text{is} & 1& 1\\[0.5em]
  \text{fun} & 1 & 1\\[0.5em]
  \text{and} & 1 & 1\\[0.5em]
  \text{matrices} & 1 & 1\\[0.5em]
  \text{applicable} & 0 & 1\\[0.5em]
  \text{useful} & 1 & 0\\[0.5em]
  \text{are} & 1 & 1
\end{array}
$$

%
%\begin{html}
% <br>
% <br>
% <table align="center" style="width:50%">
% <tr>
% <td width="25%">Mathematics</td> <td width="25%"> 1 </td> <td width="25%"> 1 </td> 
% </tr>
% <tr>
% <td width="25%">is</td> <td width="25%"> 1 </td> <td width="25%"> 1 </td> 
% </tr>
% <tr>
% <td width="25%">fun</td><td width="25%"> 1 </td> <td width="25%"> 1 </td> 
% </tr>
% <tr>
% <td width="25%">and</td> <td width="25%"> 1 </td> <td width="25%"> 1 </td> 
% </tr>
% <tr>
% <td width="25%">matrices</td> <td width="25%"> 1 </td> <td width="25%"> 1 </td> 
% </tr>
% <tr>
% <td width="25%">applicable</td> <td width="25%"> 0 </td> <td width="25%"> 1 </td> 
% </tr>
% <tr>
% <td width="25%">useful</td> <td width="25%"> 1 </td> <td width="25%"> 0 </td> 
% </tr>
% <tr>
% <td width="25%">are</td> <td width="25%"> 1 </td> <td width="25%"> 1 </td> 
% </tr>
% </table>
% <br>
% <br>
% \end{html}
where every word in the two strings has an entry counting the number of
occurences in the string. A measure for the equality between
the two strings is the cosine of the angle between the two vectors.

The closer the cosine gets to $1$ (corresponding to an angle of $0$
degrees), the more similar we consider the strings.

In the above case the cosine similarity is approximately $0.86$.

Below is a snippet of python code (using \texttt{numpy}) for computing the cosine similarity of two strings, where
words are separated by blanks. It can be extended in many ways.

% \begin{sage}
% def cosinesim(str1, str2):
%   words1 = str1.split()
%   words2 = str2.split()
%   words12 = list(set().union(words1, words2))
%   v1 = vector(ZZ, [words1.count(w) for w in words12])
%   v2 = vector(ZZ, [words2.count(w) for w in words12])
%   return (v1.inner_product(v2)/(v1.norm()*v2.norm()))

% str1 = "Mathematics is fun and matrices are useful"
% str2 = "Mathematics is fun and matrices are applicable"

% print("The cosine similarity between")
% print(str1)
% print("and")
% print(str2)
% print("is")
% print(cosinesim(str1, str2).n())
% \end{sage}

\begin{sage}
import numpy as np
from numpy.linalg import norm

def cosinesim(str1, str2):
  words1 = str1.split()
  words2 = str2.split()
  words12 = list(set().union(words1, words2))
  v1 = np.array([words1.count(w) for w in words12])
  v2 = np.array([words2.count(w) for w in words12])
  return (np.dot(v1, v2)/(norm(v1)*norm(v2)))

str1 = "Mathematics is fun and matrices are useful"
str2 = "Mathematics is fun and matrices are applicable"

print("The cosine similarity between")
print(str1)
print("and")
print(str2)
print("is")
print(cosinesim(str1, str2))
\end{sage}


This application is based on rather basic mathematics, but we do get a quantitative measure for
how close two strings are. This is a crude tool applicable for flagging potential plagiarism.

The cosine similarity is implemented in the python machine
learning library \url{\texttt{sci-kit-learn}}{https://scikit-learn.org/stable/}.

\end{example}



\subsection{Distance of vectors and the triangle inequality}

We know how to measure the size of a vector $u\in \RR^d$ by its norm $\abs{u}$. We need
to measure how close two vectors $u, v\in \RR^d$ are i.e., we
need to measure their \emph{distance}. A perfectly good
measure for the distance from $u$ to $v$ is the norm
$$
\abs{u-v}.
$$
You can see from \eqref{vecnorm} that $|u-v|$ is small implies that
the coordinates of $u$ and $v$ are close. Also we want $u = v$ if
their distance is zero. This is satisfied. Similarly we want
the distance from $u$ to $v$ to equal the distance from $v$ to $u$. This is true,
since $\abs{x} = \abs{-x}$ for any vector $x\in \RR^d$.

\beginshex
Show the above, that $\abs{x} = \abs{-x}$ for any vector $x\in \RR^d$. Explain why
this implies $\abs{u - v} = \abs{v - u}$ for every $u, v\in \RR^d$.
\endshex

One other, not so obvious property, is the \url{triangle inequality}{https://en.wikipedia.org/wiki/Triangle_inequality}.

\begin{theorem}\label{thmtrineq}
  For two vectors $u, v\in \RR^d$,
  $$
  \abs{u + v} \leq \abs{u} + \abs{v}.
  $$
  \end{theorem}
    \begin{proof}[showhide]
      From the Cauchy-Schwarz inequality (Theorem \ref{cs}) it follows that 
      $$
      \abs{u+v}^2 = (u+v)\cdot (u+v) = \abs{u}^2 + 2 u\cdot v + \abs{v}^2 \leq \abs{u}^2 + 2 \abs{u}\abs{v} + \abs{v}^2.
      $$
      Since the right hand side of this inequality is $(\abs{u} + \abs{v})^2$, the result follows.
    \end{proof}

  Why is this result called the triangle inequality? A consequence is that
  $$
  \abs{u - v}  = \abs{(u - w) + (w - v)} \leq \abs{u - w} + \abs{w - v},
  $$
  i.e., that the distance from $u$ to $v$ is always less than or equal to
  the distance from $u$ to $w$ plus the distance from $w$ to $v$, where
  $w$ is a third vector.

  In boiled down terms: the length of any one side in a triangle is
  less than or equal to the sum of the lengths of the two other sides.


\includegraphics{triangleineq.svg}

The triangle inequality implies that
\begin{align}\label{trineqopp}
  \abs{\abs{x}-\abs{y}} &\leq \abs{x - y}\\
  \abs{\abs{x}-\abs{y}} &\leq \abs{x + y}
\end{align}
for every $x, y\in \RR^d$.

\beginshex
  Show how \eqref{trineqopp} follows from Theorem \ref{thmtrineq}. 
\endshex

\section{An important remark about the real numbers}

In the beginning of this course, we postulated the existence of the real
numbers $\RR$ as an extension of the rational numbers $\QQ$ with their ordering $\leq$.

The rational numbers had the glaring defect that the graph of the function $f:\QQ\rightarrow \QQ$ given by
$$
f(x) = x^2 - 2
$$
does not intersect the $x$-axis between $1$ and $2$ in spite of the
fact that $f(1) = -1$ and $f(2) = 2$.


It seems from the sage plot below, that the graph intersects the
$x$-axis around $x_0\approx 1.4$, but it really does not happen!
Your computer and its screen only
handles rational numbers.

\begin{sage}
  plot(x**2 -2, (x, 1, 2))
\end{sage}

Surely the most natural property for a well behaved function
(like $f(x) = x^2 - 2$) is that it must intersect the $x$-axis
in a point $x_0$ with $a < x_0 < b$ if $f(a) < 0$ and $f(b) > 0$.

I will not be completely precise about how to repair this defect about the
rational numbers $\QQ$, but
just state one exceedingly important property about the real numbers $\RR$. In fact this
one property guarantees that $\RR$ does not have any holes
as in the graph above.

\subsection{Supremum}

A subset $S$ of $\RR$ is called \emph{bounded from above} if there exists
$M\in \RR$, such that $x\leq M$ for every $x\in S$. Here $M$ is
called an \emph{upper bound} for $S$.

\beginshex
Give an example of a subset of the real numbers, which is not bounded from
above and one that is.
\endshex


The real numbers satisfies that for every subset $S\subseteq \RR$ bounded from above,
 there exists a smallest upper bound denoted $\sup(S)$ called
the \emph{supremum} of $S$. In precise terms,
\begin{enumerate}[(i)]
\item
  $\sup(S)\geq x$ for every $x\in S$ 
\item
  If we move a little to the left of $\sup(S)$ we encounter
  elements from $S$: for every $\epsilon > 0$, there exists $x\in S$,
  such that
  $$
  \sup(S)-\epsilon < x \leq \sup(S).
  $$
\end{enumerate}

\includegraphics{supremum.png}

Notice that we may have $\sup(S)\not\in S$.

\subsection{Infimum}

In the same way a subset $S$ of $\RR$ is called \emph{bounded from below}, if
there exists $m\in \RR$, such that $m \leq x$ for
every $x\in S$. Every subset $S$ bounded from below has
a largest lower bound denoted $\inf(S)$ called the \emph{infimum} of $S$.
In precise terms,
\begin{enumerate}[(i)]
\item
  $\inf(S)\leq x$ for every $x\in S$ 
\item
  If we move a little to the right of $\inf(S)$ we encounter
  elements from $S$: for every $\epsilon > 0$, there exists $x\in S$,
  such that
  $$
  \inf(S) \leq x < \inf(S) + \epsilon.
  $$
\end{enumerate}


\beginshex
Give a simple example of a subset $S\subseteq \RR$ bounded from above, where
$\sup(S)\not\in S$.

Show that the subset $S = \{x\in \QQ \mid x^2 < 2\}$ of $\RR$ is bounded from above and below and
  that $\sup(S)\not\in \QQ$ and $\inf(S)\not\in \QQ$.
\endshex

\beginshex
Show that $S$ is infinite if $\sup(S)\not\in S$.
\endshex

  \section{Sequences and limits}

In order to do analysis on $\RR^d$ we need to introduce sequences of vectors.
In a sense we are going from the finite world to the infinite world. A sequence
in  $\RR^d$ is an \emph{infinite} list of vectors
$$
v_1, v_2, v_3, \dots
$$
in $\RR^d$, where repetitions are allowed. Such a sequence is denoted $(v_n)$.

In order to define a sequence we just need to tell what its $n$-th element is. So
in abstract terms a sequence in $\RR^d$ is nothing but a function $\NN\rightarrow \RR^d$.

Below we give two examples of sequences in $\RR$.
\begin{align*}
&(x_n): 1, 2, 3, 4, \dots\\
&(y_n): 1, \tfrac{1}{2}, \tfrac{1}{3}, \tfrac{1}{4}, \dots
\end{align*}
The first sequence is given by $x_n = n$ and the second $y_n = \frac{1}{n}$ for $n\in \NN$. The first sequence
explodes to infinity, whereas the second sequence gets closer and closer to $0$. In the latter case we
write
$$
\lim_{n\to\infty} y_n = 0.
$$
What does it mean that a sequence $(v_n)$ of vectors in $\RR^d$  has limit $v\in \RR^d$? Intuitively,
we can get $v_n$ as close to $v$ as we want by choosing
$n\in \NN$ sufficiently big. Here is the precise way of saying this:
\begin{equation}\label{convdef}
\forall \epsilon > 0\, \exists N\in \NN: n\geq N\implies \abs{v_n - v} < \epsilon.
\end{equation}
If a sequence $(v_n)$ has a limit $v$, then we write
$$
\lim_{n\to\infty} v_n = v.
$$

A sequence is called convergent if it has a limit. If a sequence is convergent, then it can have
only one limit. You can not have a convergent sequence with two different limits!

\beginshex
Give a precise proof of the fact that a convergent sequence $(x_n)$ can only have
one limit using proof by contradiction i.e., start by assuming that
it has two different limits $x\neq y$. Then show that 
$$
\forall \epsilon > 0\, \exists N\in \NN: n\geq N\implies \abs{x_n - y} < \epsilon.
$$
cannot be true by showing that
$$
\exists \epsilon > 0\, \forall N\in \NN: \exists n\geq N\implies \abs{x_n - y} \geq \epsilon.
$$
\begin{hint}
  Try $\epsilon = \abs{x-y}/2$ in the definition of $x$ being a limit and apply \eqref{trineqopp} to
  $$
  \abs{x_n - y} = \abs{x_n - x + x - y}.
  $$
\end{hint}
\endshex

Now, that we have the definition of a convergent sequence, we go on to use it in a rather
typical proof of a rather typical result. In this (typical) proof we first handle the
infinite and then the finite.

\begin{proposition}\label{propconvbounded}
  A convergent sequence $(x_n)$ is bounded i.e., there exists $M\in \RR$, such that
  $|x_n| \leq M$ for every $n\in \NN$.
\end{proposition}
  \begin{proof}[showhide]
    Let $x$ denote the limit of $(x_n)$. Then for $\epsilon = 1$, we may find
    $N\in \NN$, such that $\abs{x - x_n} < 1$ for $n\geq N$. Therefore
    $\abs{x_n} < \abs{x} + 1$ for $n\geq N$ by \eqref{trineqopp}. Let
    $M_1 = \max\{\abs{x_0}, \dots, \abs{x_N}\}$ and then letting
    $M = \max\{M_1, \abs{x} + 1\}$, we see that $\abs{x_n} \leq M$
    for every $n\in \NN$.
  \end{proof}



\newcommand{\phmi}{\phantom{-}}
\begin{quizexercise}[showhide]
\begin{quiz}
  \question
  What is the limit of the sequence
  $$
  \phmi 1, -1, \phmi 1, -1, \phmi 1, -1, \dots ?
  $$
  \answer{F}
  $$1$$
  \answer{F}
  $$-1$$
  \answer{T}
  It does not have a limit.
  \answer{F}
  $$0$$
\end{quiz}
\end{quizexercise}

Sage may be helpful in computing limits (see below).

\begin{sage}
n = var('n')
assume(n>0)
sequence = n/(n+1)
limit(sequence, n=infinity)
\end{sage}


For convergent sequences we have the following result.

\begin{proposition}\label{propconvarithm}
Let $(x_n)$ and $(y_n)$ be convergent sequences in $\RR^d$ with limits
$x$ and $y$ respectively. Then
\begin{enumerate}[(i)]
\item\label{itemsum}
the sequence $(x_n + y_n)$ is convergent with limit $x + y$.
\item\label{itemproduct}
the sequence $(x_n y_n)$ is convergent with limit $x y$ (if $d=1$)
\item
the sequence $(x_n/y_n)$ is convergent with limit $x/y$ provided
that $y\neq 0$ and $y_n \neq 0$ for every $n\in\NN$ (if $d=1$).
\end{enumerate}
\end{proposition}
  \begin{proof}[showhide]
    I will give the proof of \ref{itemproduct}. By definition (see
    \eqref{convdef}) we are given $\epsilon > 0$ and we must find
    $N\in \NN$, such that
    $$
    \abs{x y - x_n y_n} < \epsilon
    $$
    for $n\geq N$. An old trick shows that
    $$
    \abs{x y - x_n y_n} = \abs{(x-x_n) y + (y - y_n) x_n}.
    $$
    Therefore we may find $M > 0$ so that 
    $$
    \abs{x y - x_n y_n} \leq \abs{(x-x_n)}\abs{y} + \abs{y - y_n}\abs{ x_n} \leq \abs{x-x_n} M + \abs{y-y_n} M,
    $$
    where $y \leq M$ and $\abs{x_n}\leq M$ for every $n\in \NN$ (see Proposition \ref{propconvbounded}).
    We are assuming the $(x_n)$ and $(y_n)$ are convergent sequences. Therefore we may find
    $N_1$ and $N_2$ in $\NN$, so that
    \begin{align*}
      \abs{x - x_n} &< \frac{\epsilon}{2 M}\qquad\text{for}\qquad n \geq N_1\\
      \abs{y - y_n} &< \frac{\epsilon}{2 M}\qquad\text{for}\qquad n \geq N_2.
    \end{align*}
    Choosing $N = \max\{N_1, N_2\}$, we get
    $$
    \abs{x y - x_n y_n} \leq \abs{x-x_n} M + \abs{y-y_n} M \leq \frac{\epsilon}{2 M} M + \frac{\epsilon}{2 M} M = \epsilon,
    $$
    for $n\geq N$.
  \end{proof}



\beginshex
The proof of \ref{itemsum} in Proposition \ref{propconvarithm} is much less involved
than the given proof of \ref{itemproduct} in the same result. In the proof of
\ref{itemproduct} we used a trick using $\frac{\epsilon}{2M}$. Use the same
trick with $\frac{\epsilon}{2}$ and the triangle inequality to prove
\ref{itemsum}.
\begin{hint}
  $$
  \abs{x + y - (x_n + y_n)} = \abs{(x - x_n) + (y - y_n)} \leq \abs{x - x_n} + \abs{y - y_n}. 
  $$
\end{hint}
\endshex

\begin{quizexercise}[showhide]
  \begin{quiz}
    \question
    What is the limit of the sequence $(x_n)$ given by
    $$
    x_n = \frac{3 n^2 + 17 n + 5}{2 n^2 + 3 n + 2}?
    $$
    \answer{F}
    $$0$$
    \answer{F}
    It does not have a limit
    \answer{T}
    $$1.5$$
    \answer{F}
    $$\frac{17}{5}$$
  \end{quiz}
\end{quizexercise}


\beginshex
Consider the sequence $(x_n)$ given by
$$
x_n = \left(1 + \frac{1}{n}\right)^n.
$$
Carry out a computer experiment in sage below to find the limit
of $(x_n)$. Can you prove what you observe in
the experiment?

\begin{sage}
def exp(n):
  print("(1 + 1/n)^n is =")
  print((1 + 1/float(n))**n)
  print("for n = ", n)

exp(100)  
\end{sage}

\begin{hint}
  $$
  n \ln\left(1 + \frac{1}{n}\right) =
  \frac{\ln\left(1 + \frac{1}{n}\right) - \ln(1)}{\frac{1}{n}}
$$
\end{hint}
\endshex

%\youtube{KxIRozfLhHo}


\beginshex
Assume that $(x_n)$ is a convergent series in $\RR^d$. Show that
$(\abs{x_n})$ is a convergent sequence in $\RR$.
\endshex

\beginshex
Let $(x_n)$ be a sequence bounded below with the property that
$$
x_1 \geq x_2 \geq x_3 \geq \cdots
$$
Show that $\inf\{x_n \mid n\in \NN\}$ is the limit of $(x_n)$.

Similarly let $(z_n)$ be a sequence bounded above with the property that
$$
z_1 \leq z_2 \leq z_3 \leq \cdots
$$
Show that $\sup\{z_n \mid n\in \NN\}$ is the limit of $(z_n)$.
\endshex

\beginshex
\begin{enumerate}[(i)]
\item Show that
  $$
  \sqrt{a b} \leq \frac{a+b}{2},
  $$
  for $0\leq a \leq b$.
\item
  Prove that $a < \sqrt{a b}$ and $(a+b)/2 < b$ for $0\leq a < b$.
\item
  Start with two numbers $a$ and $b$ with $0\leq a\leq b$ and define
  \begin{align*}
    a_{n+1} &= \sqrt{a_n b_n}\\
    b_ {n+1} &= (a_n + b_n)/2,
  \end{align*}
  where $a_0 = a$ and $b_0 = b$. Carry out computer experiments in the sage (python) window below
  to analyze the sequences $a_0, a_1, \dots$ and $b_0, b_1, \dots$
  for different values of $a_0$ and $b_0$.


\begin{sage}
import math

a0 = 1
b0 = 2
print("Input a=", a0, " and b=", b0)
print("Iterating towards ag mean:")
  
for i in range(10):
  print(a0, b0)
  a1 = math.sqrt(a0*b0)
  b1 = (a0 + b0)/float(2)
  a0 = a1
  b0 = b1
\end{sage}
\item
  Prove for $n\geq 1$ that
  $$
  b_n - a_n < \left(\frac{1}{2}\right)^n (b-a)
  $$
  if $a\neq b$.
\item
  Let $s = \lim_{n\to \infty} a_n$ and $t=\lim_{n\to \infty} b_n$.
  Show that the limits exist and that $s = t$.
\end{enumerate}
The common limit $M(a, b)$ of the sequences $(a_n)$ and
$(b_n)$ is called the \url{arithmetic-geometric mean}{https://en.wikipedia.org/wiki/Arithmetic\%E2\%80\%93geometric_mean} of $a$ and $b$.
Just for the fun of it, here is a cool way of computing $\pi$
involving this quantity:
$$
\pi = \frac{4 M(1, \frac{1}{\sqrt{2}})^2}{1 - \sum_{n=1}^\infty 2^ {n+1}(b_n^2 - a_n^2)}.
$$
\endshex


\subsection{Infinite series}

Given a sequence $(x_m)$ in $\RR$ we may form the new sequence given by the sums
\begin{align*}
  s_1 &= x_1\\
  s_2 &= x_1 + x_2\\
      &\vdots\\
  s_n &= x_1 + x_2 + \cdots + x_n\\
  &\vdots       
\end{align*}
Such a sequence is called an infinite series. It is denoted
$$
\sum_{n=1}^\infty x_n
$$
and is defined to converge if the sequence $(s_n)$ converges.

Infinite series give rise to very beautiful identities like
$$
\sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}.
$$

We will not go deeper into the rich theory of infinite series, but
settle at defining a widely used infinite series called the
\emph{geometric series}.  Let $q\in \RR$ with $\abs{q} < 1$. We saw in
the first chapter that
$$
1 + q + \cdots + q^n = \frac{1 - q^{n+1}}{1-q}
$$
for any number $q\neq 1$. If $\abs{q} < 1$, then $\lim_{n\to\infty} q^n = 0$.

\beginshex
Show that $\lim_{n\to\infty} q^n = 0$ if $\abs{q} < 1$.
\endshex

Therefore
\begin{equation}\label{geomseries}
  \sum_{n=0}^\infty q^n = \frac{1}{1-q}.
\end{equation}
The series in \eqref{geomseries} is called the geometric series.

\beginshex
Compute the (infinite) sums
\begin{enumerate}[(i)]
\item
  $$
  \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots
  $$
\item
  $$
  1 - \frac{1}{2} + \frac{1}{4} - \frac{1}{8} + \cdots
  $$
\end{enumerate}
\endshex

\beginshex
The series given by $x_n = \frac{1}{n}$ i.e.,
$$
s_n = 1 + \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n}
$$
is called the \emph{harmonic series}. Explore the growth of the harmonic series as a function of $n$
using the sage window below.

\begin{sage}
n = 100
print("The sum of the first ", n , " terms in the harmonic series is ", sum(1/float(i) for i in range(1, n+1)))
\end{sage}

What does this \url{video}{https://twitter.com/TamasGorbe/status/1174406602998341632/video/1}
on twitter have to do with the harmonic series?

Suppose that the inequality
\begin{align}\label{lbharm}
\ln(n) = \int_1^n \frac{1}{x} d x \leq 1 + \frac{1}{2} + \cdots + \frac{1}{n}
\end{align}
holds. What does \eqref{lbharm} imply for the harmonic series? Is \eqref{lbharm} true? Compare
with the graphs in the sage window below.

\begin{sage}
n = 1000
  
print("Graphs of the sum of the harmonic series and the (natural) logarithm (red) up to n=", n)
x = var('x')
g = Graphics()
g += plot(harmonic_number(x), (x, 1, n))
g += plot(log(x), (x, 1, n), color='red')
g.show()
\end{sage}

Use the sage window below to investigate if the sequence $e_n$ given by
$$
e(n) = 1 + \frac{1}{2} + \cdots + \frac{1}{n} - \ln(n)
$$
converges. In particular, make a clever statement about the convergence by studying a finite table
of
$$
e(1), e(2), e(3), \dots
$$
observing $e(n) - e(n+1)$ for $n = 1, 2, 3, \dots$.

\begin{sage}
def euler(n):
  return harmonic_number(float(n)) - log(float(n))

n = 100
print("e(n) is ", euler(n), " for n=", n )
\end{sage}
\endshex

\section{Continuous functions}

\begin{definition}
A function $f: S\rightarrow T$, where $S \subseteq \RR^d$ and $T\subseteq \RR^e$ is
called \emph{continuous} if for every
convergent sequence $(v_n)$ in $S$ with limit $v\in S$, $(f(v_n))$ is a
convergent sequence with limit $f(v)$.
\end{definition}


The above is the formal definition of a continuous function. It is short and sweet. To
get an understanding, you should study the mother of all examples of 
non-continuous functions given below:
\begin{equation}\label{bracketfct}
f(x) =
\begin{cases}
  0 &\text{if } x > 0\\
  1 &\text{if } x \leq 0
\end{cases}.
\end{equation}

This is a function from $\RR$ to $\RR$. It is impossible to plot it
without lifting the pencil or defining such a beast without using a
bracket as in \eqref{bracketfct}.

\includegraphics{noncont.png}

Almost all functions we encounter will be continuous. The function $f$ above is an
anomaly.

To get continuous functions from functions already known to be continuous we 
may use the result below.

\begin{proposition}\label{contfunccomp}
Let $f, g: U\rightarrow \RR$ be functions defined on a subset $U\subset \RR^d$. If
$f$ and $g$ are continuous, then the functions
\begin{align*}
(f + g): U\rightarrow \RR\qquad\text{given by}\quad &(f + g)(x) = f(x) + g(x)\\
(f g): U\rightarrow \RR\qquad\text{given by}\quad &(f g)(x) = f(x) g(x)\\
(f/g): V\rightarrow \RR\qquad\text{given by}\quad &(f/g)(x) = f(x)/g(x)
\end{align*}
are continuous functions, where $V = \{x\in U\mid g(x)\neq 0\}$ (the last function is
defined only if $g(x)\neq 0$).
\end{proposition}

\begin{proof}[showhide]
This result is a consequence of the definition of continuity and Proposition \ref{propconvarithm}.
\end{proof}


We are now in position to prove a famous result from 1817 due til \url{Bolzano}{https://en.wikipedia.org/wiki/Bernard_Bolzano}. 

\begin{theorem}\label{Bolzano}
  Let $f: [a, b]\rightarrow \RR$ be a continuous function, where $a < b$. If
  $f(a) < 0$ and $f(b) > 0$, then there exists $x_0$ with $a < x_0 < b$, such
  that $f(x_0) = 0$.
\end{theorem}

  \begin{proof}[showhide]
    This is proved using the supremum property of the real numbers. The subset
    $$
    S = \{x\in [a, b]\mid f(x) \leq 0\}
    $$
    is non-empty (since $a\in S$) and bounded from above. We let $c = \sup(S)$.

    We will need the following observation about the continuous function $f$:
    If $f(z) < 0$ for $a \leq z < b$, then there exists a small
    $\delta > 0$, such that
    $$
    f(x) < 0
    $$
    for every $x\in [z, z+\delta]$.

    Similarly if $f(z) > 0$ for $a <  z \leq b$, then there exists a small
    $\delta > 0$, such that
    $$
    f(x) > 0
    $$
    for every $x\in [z - \delta, z]$.

    These observations imply that $a < c < b$ by the definition
    of supremum. Similarly we cannot according to these
    observations have $f(c) < 0$ or $f(c) > 0$.

    The only possibility remaining is $f(c) = 0$.
  \end{proof}

By Proposition \ref{contfunccomp}, polynomials are continuous functions.

Now, as promised previously, we state and prove the following result.

\begin{proposition}
Let
$$
f(x) = a_n x^n + \cdots + a_1 x + a_0
$$
be a polynomial of odd degree, i.e. $n$ is odd. Then $f$ has a root,
i.e. there exists $x_0\in\RR$, such that $f(x_0) = 0$.
\end{proposition}
\begin{proof}[showhide]
We will assume that $a_n > 0$ (if not, just multiply $f$ by $-1$). Consider $f(x)$ written as
$$
f(x) = x^n \left(a_n + \frac{a_{n-1}}{x} + \cdots + \frac{a_1}{x^{n-1}} + \frac{a_0}{x^n}\right).
$$
By choosing $c$ negative with $\abs{c}$ extremely big, we have $f(c) < 0$, 
since $c^n$ is negative and 
$$
a_n + \frac{a_{n-1}}{c} + \cdots + \frac{a_1}{c^{n-1}} + \frac{a_0}{c^n} > 0
$$
as $a_n$ is positive. Notice here that the terms
$$
\frac{a_{n-1}}{c} + \cdots + \frac{a_1}{c^{n-1}} + \frac{a_0}{c^n} 
$$
are extremely small, when $\abs{c}$ is extremely big.

Similarly by choosing
$d$ positive and tremendously big, we have $f(d) > 0$.
By Theorem \ref{Bolzano}, there exists $x_0$ with $c < x_0 < d$ with
$f(x_0) = 0$.
\end{proof}



\section{Special subsets}



\subsection{Preimages}

Before defining (and more importantly giving examples of) closed subsets, we will
define abstractly the preimage of a subset of a function. So consider a
function
$$
f: A \rightarrow B,
$$
where $A$ and $B$ are sets. If $C\subseteq B$ is a subset of $B$, then the
preimage of $B$ under $f$ is defined by
$$
f^{-1}(C) = \{x\in A \mid f(x)\in C\}.
$$

\subsection{Closed subsets}

A subset $F\subseteq \RR^d$ is called closed if it contains all its limit vectors. This
means that if $(v_n)$ is a convergent sequence contained in $F$, then its limit must
be contained in $F$.

We can immediately come up with a non-closed subset using the definition. Consider the subset
$$
S = \{x\in \RR \mid x > 0\}\subset \RR.
$$
Here $\left(\frac{1}{n}\right)$ is a convergent sequence, whose elements all are
contained in $S$, but its limit $0$ is outside $S$.
We have, however, the following important result relating to this example.

\begin{proposition}
  The following subsets
  \begin{align*}
    [a, b] &= \{x\in \RR \mid a \leq x \leq b\} \\
    [a, \infty) &= \{x\in \RR \mid a \leq x\}\\
    (-\infty, a] &= \{x\in \RR \mid x \leq a\}
  \end{align*}
  are closed subsets of $\RR$ for every $a, b\in \RR$.
\end{proposition}

Also, important are the following two results.

\begin{proposition}
  Let $F_1, F_2, \dots, F_n$ be finitely many closed subsets of $\RR^d$. Then their
  intersection
  $$
  F_1 \cap F_2 \cap \cdots \cap F_n
  $$
  is a closed subset of $\RR^d$.
\end{proposition}

\begin{proposition}[emph]
  If $F\subseteq \RR^e$ is a closed subset and $f: \RR^d\rightarrow \RR^e$
  a continuous function, then the preimage
  $$
  f^{-1}(F)
  $$
  is a closed subsets of $\RR^d$.
\end{proposition}

\begin{example}
  The function $f:\RR^2\rightarrow \RR$ given by $f(x, y) = x^2 + y^2$ is
  continuous. Therefore the set
  $$
  \{(x, y)\in \RR^2\mid x^2 + y^2 \geq 1\} = f^{-1}([1, \infty))
  $$
  is closed.
  \end{example}


  \subsection{Open subsets}

  A subset $U\subseteq \RR^d$ is called \emph{open} if $\RR^d\setminus U$ is closed.


  \beginshex
  Let $(a, b) =\{x \in \RR \mid a < x < b\}$ for $a, b\in \RR$, where $a < b$.
  Show that $(a, b)$ is an open subset of $\RR$.
  \endshex
  
  We have the following result, which we will not prove.

\begin{theorem}\label{convexcont}
  Let $O\subseteq \RR^d$ be an open convex subset. Then a convex function $f:O\rightarrow \RR$
  is continuous.
\end{theorem}

\beginshex
Give an example for $d=1$ that Theorem \ref{convexcont} fails if $O$ is not
assumed open.
\endshex



\subsection{Convex closed subsets}

Closed convex subsets are quite special. They have the following
interesting property, which we do not have time (unfortunately) to prove.

\begin{theorem}\label{Thm:seppoint}
  Let $C\subseteq \RR^d$ be a closed convex subset and $y\in \RR^d$. Then
  there exists a point $x_0\in C$ closest to $y$ i.e.,
  \begin{equation*}
    \abs{ x_0 - y } \leq \abs{ x - y }
  \end{equation*}
  for every $x\in C$. This point is uniquely given by the property
  that
  \begin{equation*}
    (x - x_0)\cdot (x_0 - y) \geq 0
  \end{equation*}
  for every $x\in C$.
\end{theorem}

\includegraphics{convexclosest.png}  

One may readily apply this result to prove that if $y\not\in C$, then there exists
a hyperplane given by $\alpha^T x = \beta$ for $\alpha\in\RR^d\setminus\{0\}$ and
$\beta\in \RR$, such that
$\alpha^T y < \beta$ and 
$$
\alpha^T x > \beta
$$
for every $x\in C$.

\beginshex
Apply Theorem \ref{Thm:seppoint} to prove the
above existence of $\alpha\in \RR^d\setminus\{0\}$ and $\beta\in \RR$ by
using $\alpha = x_0 - y$ and
$$
(x - x_0)\cdot (x_0 - y) = (x-\alpha -y)\cdot \alpha = \alpha^T x - \abs{\alpha}^2 -\alpha^T y\geq 0
$$
for every $x\in C$. Show that $\beta = \alpha^T y + \abs{\alpha}^2/2$ works i.e. for these
values of $\alpha$ and $\beta$ we really do have
$\alpha^T y < \beta$ and 
$$
\alpha^T x > \beta
$$
for every $x\in C$.
\endshex

\subsection{Bounded subsets}

A subset $S \subseteq \RR^d$ is called bounded if there is an upper limit on the norm
of the vectors contained in it i.e., there must exist a  positive number $M$, such that
$$
\abs{u}\leq M,
$$
for every $u\in S$. The subset $\NN$ of $\RR$ is not bounded, whereas the subset
$\{1, \frac{1}{2}, \frac{1}{3}, \dots\}$ is.




\subsection{Compact subsets and a basic optimization result}\label{sectcompact}

A subset is called compact if it is bounded and closed.


\begin{theorem}[emph]
  Let $K$ be a compact subset of $\RR^d$ and $f: K\rightarrow \RR$
  a continuous function. Then there exists $v\in K$, such that
  $$
  f(v) \leq f(x),
  $$
  for every $x\in K$.
\end{theorem}

This is a rather stunning result! You are guaranteed solutions to
optimization problems of the type

\begin{align*}
  &\text{Minimize} &f(x)&\\
  &\text{with constraint}\\
  &&x\in K,
\end{align*}

where $K$ is a compact subset and $f: K\rightarrow \RR$ a continuous function.



\section{The perceptron algorithm}\label{sectperceptron}


The perceptron algorithm is a fancy name for a surprisingly simple
algorithm for computing a hyperplane that divides binary labeled
points.

We will assume that $x_1, \dots, x_n$ are
points in $\RR^d$ each of which is labeled by
a number $l_i\in \{-1, 1\}$.

Assuming that you can find a (separating) hyperplane $\alpha^T v = \beta$,
such that
$$
\alpha^T x_i > \beta\qquad\text{if}\qquad l_i = 1
$$
and
$$
\alpha^T x_i < \beta\qquad\text{if}\qquad l_i = -1.
$$


\includegraphics{perceptronsephyp.png}


We are interested in given the points and their labels in
computing $\alpha\in \RR^d$ and $\beta\in \RR$. In machine learning
the term \emph{training the algorithm} is used pointing
towards using the hyperplane for future points not in
the initial data set.


\subsection{Add a dimension to make it easier}

An extremely useful trick is to add one more dimension
and study the problem in $\RR^{d+1}.$ We replace
each point

$$
x_i\in \RR^d\qquad\text{ by }\qquad \hat{x_i} = (l_i x_i, l_i)\in \RR^{d+1}.
$$

The original problem is now equivalent to finding $\hat{\alpha}\in \RR^{d+1}\setminus\{0\}$,
such that
\begin{equation}\label{liftproblem}
\hat{\alpha}^T \hat{x_i} > 0
\end{equation}
for every $i = 1, \dots, n$. If such an $\hat{\alpha} = (\alpha_1, \dots, \alpha_d, t)^T$ can
be found, then we may take $\alpha = (\alpha_1, \dots, \alpha_d)^T$ and $\beta = -t$ as
a solution to our original problem.
On the other hand, if $\alpha$ and $\beta$ solve our original problem, then
$\hat{\alpha} = (\alpha, -\beta)$ solves \eqref{liftproblem}.


We have reduced our original problem to a simpler problem in
one more dimension. We will focus on this problem (now $d+1$ is
replaced by $d$):

Suppose that we are given points $x_1, \dots, x_n\in \RR^d$ and
there exists $\alpha\in \RR^d\setminus \{0\}$, such that
$\alpha^T x_i > 0$ for every $i = 1, \dots, n$. Geometrically
this means that all the points are on one side of the
hyperplane $\alpha^T v = 0$.

\includegraphics{perceptronsephpplusone.png}


We are interested in computing such an $\alpha$. This is exactly
what is done by the simple
algorithm presented in Example \ref{exampleperceptronsimple}.

\begin{example}\label{Examplepercep}
Consider the points
$$
v_1 = \vect{0}{0},\qquad v_2=\vect{1}{1}\qquad\text{and}\qquad v_3=\vect{\hphm 1}{-1}
$$
in $\RR^2$, where $v_1$ and $v_2$ are labeled by $+1$ and $v_3$ is labeled by $-1$. Then
$$
\hat{v}_1 = \vects{0}{0}{1},\qquad \hat{v}_2 = \vects{1}{1}{1}\qquad
\text{and}\qquad \hat{v}_3 = \vects{-1}{\hphm 1}{-1}.
$$
Now we run the simple algorithm from Example \ref{exampleperceptronsimple}:
$$
\hat{\alpha} = \vects{0}{0}{0}\xrightarrow{+\hat{v}_1} \vects{0}{0}{1} \xrightarrow{+\hat{v}_3} \vects{-1}{\hphm 1}{\hphm 0} \xrightarrow{+\hat{v}_1} \vects{-1}{\hphm 1}{\hphm 1}.
$$

From the last vector we see that $\alpha = \vect{-1}{\hphm 1}$ and $\beta = -1$ determine
a separating hyperplane (line) of the labeled points.

\includegraphics{perceptronplane.png}

\end{example}


\beginshex
Consider the points
$$
\begin{pmatrix}
0 \\ 0
\end{pmatrix},\qquad
\begin{pmatrix}
0 \\ 1
\end{pmatrix},\qquad

\begin{pmatrix}
1 \\ 1
\end{pmatrix},\qquad\text{and}\qquad
\begin{pmatrix}
1 \\ 0
\end{pmatrix}
$$
in $\RR^2$, where the first point is labeled with $-1$ and the rest by $1$.
Use the perceptron algorithm to compute a separating hyperplane.

What happens when you run the perceptron algorithm on the above
points, but where the label of
$$
\begin{pmatrix}
  1 \\ 1
\end{pmatrix}
$$
is changed from $1$ to $-1$?
\endshex


\begin{hideinbutton}{Why does the simple algorithm work?}
  We will assume that there exists $\alpha\in \RR^d$, such that
  $$
  \alpha^T x_i > 0
  $$
  for every $i = 1, \dots, n$. This is equivalent to the existence of $\alpha^*\in \RR^d$, such that
  $$
  (\alpha^*)^T x_i \geq 1
  $$
  for every $i = 1, \dots, n$. Let $r = \max\{\abs{x_1}, \dots, \abs{x_n}\}$. The basic insight
  is the following
  \begin{proposition}\label{proprepl}
    After $m$ replacements of $\alpha$ in the algorithm, we have
    \begin{align*}
      \alpha^T \alpha^* &\geq m \\
      m r^2 &\geq \abs{\alpha}^2.
    \end{align*}
  \end{proposition}
  \begin{proof}[showhide]
    These statements follow from the inequalities
    $$
    (\alpha + x_i)^T \alpha^* = \alpha^T \alpha^* + x_i^T \alpha^* \geq \alpha^T \alpha^* + 1
    $$
    and
    $$
    (\alpha + x_i)^T (\alpha + x_i) = \abs{\alpha}^2 + 2 x_i^T \alpha + \abs{x_i}^2 \leq \abs{\alpha}^2 +
    \abs{x_i}^2 \leq \abs{\alpha}^2 + r^2.
    $$
  \end{proof}

  Proposition \ref{proprepl} implies that
$$
m \leq \abs{\alpha} \abs{\alpha^*} \leq \sqrt{m} r \abs{\alpha^*}.
$$
Therefore we get $m\leq r^2 \abs{\alpha^*}^2$ and there is an upper bound on the number of
iterations used in the second step. At a certain iteration within this bound
we must have $\alpha^T x_i > 0$ for every $i=1, \dots, n$.
\end{hideinbutton}

Below is an implementation of the perceptron (learning) algorithm
in python with input from Example \ref{Examplepercep}.

\begin{sage}
import numpy as np

vectors = [[0,0], [1,1], [1,-1]]
labels = [1, 1, -1]

def liftvectors(vectors, labels):
    return [list(map(lambda t: x[1]*t, x[0]))+[x[1]] for x in zip(vectors, labels)]

def perceptron(inputvectors):
    alpha = np.array(len(inputvectors[0])*[0])
    vectors = list(map(np.array, inputvectors))
    done = False
    while not done:
        done = True
        for v in vectors:
            if np.dot(alpha, v) <= 0:
               alpha = np.add(alpha, v)  
               done = False
               break
    return alpha

print("The (normal) vector alpha in dimension one more is:")
    
perceptron(liftvectors(vectors, labels))    
\end{sage}

\beginshex
For python aficionados: why does the above code stop working when
\begin{quote}
  \texttt{vectors = list(map(np.array, inputvectors))}
\end{quote}
is replaced by
\begin{quote}
  \texttt{vectors = map(np.array, inputvectors)}
\end{quote}
?
\endshex

\end{document}